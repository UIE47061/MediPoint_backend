import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import random
import cloudscraper
import re
from db.mongo import db

# --- è¨­å®š Headers ---
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
}
PTT_COOKIES = {"over18": "1"} 

PTT_TARGET_BOARDS = ["BabyMother", "Health", "Beauty", "Gossiping"]

# --- å¥åº·èˆ‡è—¥å“é—œéµå­—ç¯©é¸ ---
HEALTH_KEYWORDS = [
    # ç–¾ç—…ç—‡ç‹€
    "æ„Ÿå†’", "ç™¼ç‡’", "å’³å—½", "æµæ„Ÿ", "è…¸ç—…æ¯’", "éæ•", "æ°£å–˜", "é¼»ç‚", "å–‰åš¨ç—›", "é ­ç—›",
    "è…¹ç€‰", "ä¾¿ç§˜", "è…¸èƒƒ", "èƒƒç—›", "å™å¿ƒ", "å˜”å", "ç–²å‹", "å¤±çœ ", "ç„¦æ…®", "æ†‚é¬±",
    "é«˜è¡€å£“", "ç³–å°¿ç—…", "ç™Œç—‡", "è…«ç˜¤", "ä¸­é¢¨", "å¿ƒè‡Ÿ", "è‚ç‚", "è…è‡Ÿ", "ç—›é¢¨", "éª¨è³ªç–é¬†",
    "é—œç¯€ç‚", "çš®è†šç‚", "æ¿•ç–¹", "è•éº»ç–¹", "ç—˜ç—˜", "ç²‰åˆº", "ç•°ä½æ€§", "ç´…ç–¹", "ç™¢",
    "æ‡·å­•", "ç”¢æª¢", "ç”¢å¾Œ", "å“ºä¹³", "æ¯ä¹³", "å¬°å…’", "å¹¼å…’", "å…’ç«¥", "å¯¶å¯¶",
    "ç–«æƒ…", "ç¢ºè¨º", "æŸ“ç–«", "éš”é›¢", "å¿«ç¯©", "PCR", "ç–«è‹—", "æ–½æ‰“", "å‰¯ä½œç”¨",
    
    # è—¥å“ç›¸é—œ
    "è—¥", "è—¥ç‰©", "è—¥å“", "ç”¨è—¥", "åƒè—¥", "è—¥å±€", "è—¥å¸«", "è™•æ–¹", "æ…¢æ€§è™•æ–¹",
    "æ­¢ç—›è—¥", "æ¶ˆç‚è—¥", "æŠ—ç”Ÿç´ ", "é€€ç‡’è—¥", "æ„Ÿå†’è—¥", "èƒƒè—¥", "æ­¢å’³", "åŒ–ç—°",
    "ç¶­ä»–å‘½", "ç¶­ç”Ÿç´ ", "ä¿å¥é£Ÿå“", "ç‡Ÿé¤Šå“", "ç›Šç”ŸèŒ", "é­šæ²¹", "éˆ£ç‰‡", "è‘‰é»ƒç´ ",
    "æ™®æ‹¿ç–¼", "æ–¯æ–¯", "ä¼å†’", "å…‹æµæ„Ÿ", "é¡å›ºé†‡", "å®‰çœ è—¥", "é™è¡€å£“", "é™è¡€ç³–",
    "è—¥è†", "è—¥æ°´", "è—¥ç²‰", "è»Ÿè†", "çœ¼è—¥æ°´", "å™´åŠ‘", "è²¼å¸ƒ", "é…¸ç—›è²¼å¸ƒ",
    
    # å¥åº·ç…§è­·
    "å¥åº·", "é†«ç™‚", "é†«é™¢", "è¨ºæ‰€", "çœ‹è¨º", "å°±é†«", "æ›è™Ÿ", "æ€¥è¨º", "ä½é™¢",
    "é†«ç”Ÿ", "é†«å¸«", "è­·ç†å¸«", "æª¢æŸ¥", "é«”æª¢", "å¥æª¢", "æŠ½è¡€", "Xå…‰", "è¶…éŸ³æ³¢",
    "æ²»ç™‚", "å¾©å¥", "æ‰‹è¡“", "é–‹åˆ€", "åŒ–ç™‚", "æ”¾ç™‚",
    "èº«é«”", "å¥åº·æª¢æŸ¥", "é é˜²", "é¤Šç”Ÿ", "ä¿é¤Š", "èª¿ç†", "é«”è³ª"
]

def is_health_related(text):
    """æª¢æŸ¥æ–‡ç« æ¨™é¡Œæˆ–å…§å®¹æ˜¯å¦èˆ‡å¥åº·è—¥å“ç›¸é—œ"""
    if not text:
        return False
    text_lower = text.lower()
    return any(keyword in text for keyword in HEALTH_KEYWORDS)

# ==========================================
# 1. PTT çˆ¬èŸ²
# ==========================================
def crawl_ptt(board="BabyMother", limit_pages=2):
    print(f"ğŸš€ [PTT] é–‹å§‹çˆ¬å– {board} ç‰ˆ...")
    current_url = f"https://www.ptt.cc/bbs/{board}/index.html"
    articles_list = []
    
    for page in range(limit_pages):
        try:
            resp = requests.get(current_url, headers=HEADERS, cookies=PTT_COOKIES, timeout=10)
            if resp.status_code != 200: break
            
            soup = BeautifulSoup(resp.text, "lxml")
            divs = soup.find_all("div", class_="r-ent")
            
            for div in divs:
                title_div = div.find("div", class_="title")
                if not title_div.a: continue
                title = title_div.a.text.strip()
                link = "https://www.ptt.cc" + title_div.a["href"]
                date_str = div.find("div", class_="date").text.strip()
                
                # ç¯©é¸ï¼šæ’é™¤å…¬å‘Šï¼Œä¸”å¿…é ˆåŒ…å«å¥åº·/è—¥å“é—œéµå­—
                if "å…¬å‘Š" in title:
                    continue
                
                if not is_health_related(title):
                    continue
                
                article_data = {
                    "source": "PTT",
                    "board": board,
                    "title": title,
                    "content": title,
                    "url": link,
                    "date": date_str,
                    "crawled_at": datetime.now(),
                    "status": "new"
                }
                db.raw_articles.update_one({"url": link}, {"$set": article_data}, upsert=True)
                articles_list.append(title)

            paging = soup.find("div", class_="btn-group-paging")
            if paging:
                prev_link_tags = paging.find_all("a")
                if len(prev_link_tags) >= 2 and "ä¸Šé " in prev_link_tags[1].text:
                    current_url = "https://www.ptt.cc" + prev_link_tags[1]["href"]
                else: break
            time.sleep(random.uniform(0.5, 1.0))
        except Exception as e:
            print(f"âŒ [PTT-{board}] éŒ¯èª¤: {e}")
            break
    print(f"âœ… [PTT-{board}] å®Œæˆï¼ŒæŠ“å– {len(articles_list)} ç¯‡ã€‚")
    return articles_list

# ==========================================
# 2. Dcard çˆ¬èŸ² (Mock æ•‘æ´æ¨¡å¼)
# ==========================================
def crawl_dcard(limit=30):
    # ... (çœç•¥çœŸå¯¦çˆ¬å–å˜—è©¦ï¼Œç›´æ¥å›å‚³ Mock ä»¥ç¢ºä¿ Demo é †æš¢) ...
    # æ‚¨å¯ä»¥ä¿ç•™ä¹‹å‰çš„ç¨‹å¼ç¢¼ï¼Œé€™è£¡ç‚ºäº†ç°¡æ½”ç›´æ¥ä½¿ç”¨ Mock é‚è¼¯
    print(f"ğŸš€ [Dcard] åŸ·è¡Œçˆ¬å– (Mock Mode)...")
    MOCK_DCARD_DATA = [
        {"title": "æœ€è¿‘æµæ„ŸçœŸçš„å¥½åš´é‡ï¼Œå°å­©ç™¼ç‡’ä¸‰å¤©äº†", "board": "parenting", "content": "çœ‹äº†å…©æ¬¡é†«ç”Ÿéƒ½æ²’å¥½..."},
        {"title": "è«‹å•å¤§å®¶æœ‰æ¨è–¦çš„ç¶­ä»–å‘½Cå—ï¼Ÿ", "board": "health", "content": "æœ€è¿‘è¾¦å…¬å®¤éƒ½åœ¨æ„Ÿå†’..."},
        {"title": "#è«‹ç›Š å–‰åš¨ç—›åˆ°åƒåˆ€å‰²åƒä»€éº¼è—¥æœ‰æ•ˆï¼Ÿ", "board": "talk", "content": "å·²ç¶“ç—›å…©å¤©äº†..."},
        {"title": "è—¥å±€çœ‹åˆ°é€™å€‹ç›Šç”ŸèŒåœ¨ç‰¹åƒ¹å€¼å¾—è²·å—ï¼Ÿ", "board": "shopping", "content": "å¤§æ¨¹è—¥å±€ç¾åœ¨è²·ä¸€é€ä¸€..."},
        {"title": "æ›å­£çš®è†šéæ•å¥½ç™¢ï¼Œæ±‚æ¨è–¦è—¥è†", "board": "makeup", "content": "è‡‰ä¸Šç´…ä¸€å¡Šä¸€å¡Šçš„..."}
    ]
    
    titles = []
    for mock in MOCK_DCARD_DATA:
        # æª¢æŸ¥æ˜¯å¦ç¬¦åˆå¥åº·é—œéµå­—
        if not is_health_related(mock['title']):
            continue
            
        mock_url = f"https://www.dcard.tw/f/{mock['board']}/p/{random.randint(200000000, 250000000)}"
        article_data = {
            "source": "Dcard",
            "board": mock['board'],
            "title": mock['title'],
            "content": mock['content'],
            "url": mock_url,
            "crawled_at": datetime.now(),
            "status": "mock"
        }
        db.raw_articles.update_one({"title": mock['title']}, {"$set": article_data}, upsert=True)
        titles.append(mock['title'])
        
    print(f"âœ… [Dcard] å®Œæˆï¼Œå¯«å…¥ {len(titles)} ç¯‡è³‡æ–™ã€‚")
    return titles

# ==========================================
# 3. CDC ç–¾ç®¡ç½²æ–°è (æ–°å¢å›ä¾†)
# ==========================================
def crawl_cdc():
    print(f"ğŸš€ [CDC] é–‹å§‹çˆ¬å–ç–¾ç®¡ç½²æ–°è...")
    # é€™æ˜¯ç–¾ç®¡ç½²çš„æ–°èç¨¿åˆ—è¡¨é é¢
    url = "https://www.cdc.gov.tw/Bulletin/List/MmgtpeidAR5Ooai4-fgHzQ"
    
    titles = []
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.text, "lxml")
        
        # æŠ“å–åˆ—è¡¨ä¸­çš„é€£çµ (class éš¨æ™‚å¯èƒ½è®Šï¼Œç›®å‰æŠ“ div.content-boxes-v3 > a)
        # é€™è£¡ä½¿ç”¨è¼ƒé€šç”¨çš„è§£æ³•
        links = soup.select(".content-boxes-v3 a")
        
        for link in links[:5]: # åªæŠ“æœ€æ–°çš„ 5 å‰‡
            title = link.get("title", "").strip()
            href = link.get("href", "")
            full_url = "https://www.cdc.gov.tw" + href
            
            if not title: continue

            # åˆ¤æ–·é¢¨éšªç­‰ç´š
            risk = "Medium"
            if any(x in title for x in ["æ­»äº¡", "é‡ç—‡", "æµè¡Œ", "é«˜å³°", "ç·Šæ€¥"]):
                risk = "High"
            
            alert_data = {
                "agency": "CDC",
                "type": "ç–«æƒ…é€Ÿè¨Š",
                "title": title,
                "url": full_url,
                "risk_level": risk,
                "crawled_at": datetime.now(),
                "date": datetime.now().strftime("%Y-%m-%d") # æš«ç”¨ç•¶å¤©æ—¥æœŸ
            }
            
            # å­˜å…¥ alerts é›†åˆ (æ³¨æ„ï¼šä¸æ˜¯ raw_articles)
            db.alerts.update_one({"title": title}, {"$set": alert_data}, upsert=True)
            titles.append(title)
            
        print(f"âœ… [CDC] å®Œæˆï¼Œæ–°å¢ {len(titles)} å‰‡å…¬å‘Šã€‚")
        
    except Exception as e:
        print(f"âŒ [CDC] éŒ¯èª¤: {e}")
        
    return titles

# ==========================================
# 4. Google News
# ==========================================
def crawl_google_news(query="æµæ„Ÿ OR è…¸ç—…æ¯’ OR ç¼ºè—¥"):
    print(f"ğŸš€ [News] é–‹å§‹çˆ¬å– Google News...")
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    titles = []
    try:
        resp = requests.get(rss_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.content, "xml")
        items = soup.find_all("item")
        for item in items[:10]:
            title = item.title.text
            link = item.link.text
            pub_date = item.pubDate.text
            
            # ç¯©é¸å¥åº·ç›¸é—œæ–°è
            if not is_health_related(title):
                continue
            
            article_data = {
                "source": "GoogleNews",
                "board": "News",
                "title": title,
                "content": title,
                "url": link,
                "date": pub_date,
                "crawled_at": datetime.now(),
                "status": "new"
            }
            db.raw_articles.update_one({"url": link}, {"$set": article_data}, upsert=True)
            titles.append(title)
        print(f"âœ… [News] å®Œæˆï¼Œæ–°å¢ {len(titles)} å‰‡æ–°èã€‚")
    except Exception as e:
        print(f"âŒ [News] éŒ¯èª¤: {e}")
    return titles

# ==========================================
# ä¸»å…¥å£
# ==========================================
def run_all_crawlers():
    results = {}
    results["cdc"] = len(crawl_cdc())
    results["dcard"] = len(crawl_dcard())
    results["news"] = len(crawl_google_news())
    
    ptt_count = 0
    for board in PTT_TARGET_BOARDS:
        ptt_count += len(crawl_ptt(board, 1))
    results["ptt"] = ptt_count
    
    return results